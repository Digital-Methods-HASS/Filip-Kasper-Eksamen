---
title: "GoT"
author: "Filip B. Rasmussen and Kasper N. Rasmussen"
date: "2025-03-19, updated 2025-05-16"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)

library(tidyverse)
library(here)

# For text mining:
library(pdftools)
library(tidytext)
library(textdata) 
library(ggwordcloud)

```


### Get the Got report:
```{r get-document}
Got_path <- here("data","got.pdf")
Got_path
Got_text <- pdf_text(Got_path)
# Got_text
```

```{r single-page}
Got_p9 <- Got_text[9]
Got_p9
```


```{r split-lines}
Got_df <- data.frame(Got_text) %>% 
  mutate(text_full = str_split(Got_text, pattern = '\n')) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full)) 


Got_df
```

Now each line, on each page, is its own row, with extra starting & trailing spaces removed. 

### Get the tokens (individual words) in tidy format

```{r tokenize}
Got_tokens <- Got_df %>% 
  unnest_tokens(word, text_full)
Got_tokens

# See how this differs from `Got_df`
# Each word has its own row!
```

Let's count the words!
```{r count-words}
Got_wc <- Got_tokens %>% 
  count(word) %>% 
  arrange(-n)
Got_wc
```

OK...so we notice that a whole bunch of things show up frequently that we might not be interested in ("a", "the", "and", etc.). These are called *stop words*. Let's remove them. 

### Remove stop words:

We will *remove* stop words using `tidyr::anti_join()`:
```{r stopwords}
View(stop_words)
stop_words

Got_stop <- Got_tokens %>% 
  anti_join(stop_words) %>% 
  select(-Got_text)
```

Now check the counts again: 
```{r count-words2}
Got_swc <- Got_stop %>% 
  count(word) %>% 
  arrange(-n)
Got_swc
```

What if we want to get rid of all the numbers (non-text) in `Got_stop`?
```{r skip-numbers}
# This code will filter out numbers by asking:
# If you convert to as.numeric, is it NA (meaning those words)?
# If it IS NA (is.na), then keep it (so all words are kept)
# Anything that is converted to a number is removed

Got_no_numeric <- Got_stop %>% 
  filter(is.na(as.numeric(word)))
```

### A word cloud of Got report words (non-numeric)


```{r wordcloud-prep}
# There are over 11000 unique words 
length(unique(Got_no_numeric$word))

# We probably don't want to include them all in a word cloud. Let's filter to only include the top 100 most frequent?
Got_top100 <- Got_no_numeric %>% 
  count(word) %>% 
  arrange(-n) %>% 
  head(100)
```

```{r wordcloud}
Got_cloud <- ggplot(data = Got_top100, aes(label = word)) +
  geom_text_wordcloud() +
  theme_minimal()

Got_cloud
```

That's underwhelming. Let's customize it a bit:
```{r wordcloud-pro}
ggplot(data = Got_top100, aes(label = word, size = n)) +
  geom_text_wordcloud_area(aes(color = n), shape = "diamond") +
  scale_size_area(max_size = 12) +
  scale_color_gradientn(colors = c("forestgreen","navyblue","orange")) +
  theme_minimal()
```


### Sentiment analysis

"afinn": Words ranked from -5 (very negative) to +5 (very positive)
```{r afinn}
get_sentiments(lexicon = "afinn")
# Note: may be prompted to download (yes)

# Let's look at the pretty positive words:
afinn_pos <- get_sentiments("afinn") %>% 
  filter(value %in% c(3,4,5))

# Do not look at negative words in class. 
afinn_pos
```

bing: binary, "positive" or "negative"
```{r bing}
get_sentiments(lexicon = "bing")
```



Now nrc:
```{r nrc}
get_sentiments(lexicon = "nrc")
```

Let's do sentiment analysis on the Got text data using afinn, and nrc. 


### Sentiment analysis with afinn: 

First, bind words in `Got_stop` to `afinn` lexicon:
```{r bind-afinn}
Got_afinn <- Got_stop %>% 
  inner_join(get_sentiments("afinn"))
Got_afinn
```

Let's find some counts (by sentiment ranking):
```{r count-afinn}
Got_afinn_hist <- Got_afinn %>% 
  count(value)

# Plot them: 
ggplot(data = Got_afinn_hist, aes(x = value, y = n)) +
  geom_col(aes(fill = value))+
  theme_bw()
```

Investigate some of the words in a bit more depth:
```{r afinn-2}
# What are these '2' words?
Got_afinn2 <- Got_afinn %>% 
  filter(value == 2)
```

```{r afinn-2-more}
# Check the unique 2-score words:
unique(Got_afinn2$word)

# Count & plot them
Got_afinn2_n <- Got_afinn2 %>% 
  count(word, sort = TRUE) %>% 
  mutate(word = fct_reorder(factor(word), n))


ggplot(data = Got_afinn2_n, aes(x = word, y = n)) +
  geom_col() +
  coord_flip()+
  theme_bw()

 
```



We can summarize sentiment for the report: 
```{r summarize-afinn}
Got_summary <- Got_afinn %>% 
  summarize(
    mean_score = mean(value),
    median_score = median(value)
  )
Got_summary
```

The mean and median indicate *slightly* negative overall sentiments based on the AFINN lexicon. 

### NRC lexicon for sentiment analysis

We can use the NRC lexicon to start "binning" text by the feelings they're typically associated with. As above, we'll use inner_join() to combine the Got non-stopword text with the nrc lexicon: 

```{r bind-bing}
Got_nrc <- Got_stop %>% 
  inner_join(get_sentiments("nrc"))
```

Wait, won't that exclude some of the words in our text? YES! We should check which are excluded using `anti_join()`:

```{r check-exclusions}
Got_exclude <- Got_stop %>% 
  anti_join(get_sentiments("nrc"))

# View(ipcc_exclude)

# Count to find the most excluded:
Got_exclude_n <- Got_exclude %>% 
  count(word, sort = TRUE)

head(Got_exclude_n)
```

**Lesson: always check which words are EXCLUDED in sentiment analysis using a pre-built lexicon! **

Now find some counts: 
```{r count-bing}
Got_nrc_n <- Got_nrc %>% 
  count(sentiment, sort = TRUE)

# And plot them:

ggplot(data = Got_nrc_n, aes(x = sentiment, y = n)) +
  geom_col(aes(fill = sentiment))+
  theme_bw()
```

Or count by sentiment *and* word, then facet:
```{r count-nrc}
Got_nrc_n5 <- Got_nrc %>% 
  count(word,sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

Got_nrc_gg <- ggplot(data = Got_nrc_n5, aes(x = reorder(word,n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "count")

# Show it
Got_nrc_gg

# Save it
ggsave(plot = Got_nrc_gg, 
       here("figures","Got_nrc_sentiment.png"), 
       height = 8, 
       width = 5)

```

Wait, so "confidence" is showing up in NRC lexicon as "fear"? Let's check:
```{r nrc-confidence}
conf <- get_sentiments(lexicon = "nrc") %>% 
  filter(word == "confidence")

# Yep, check it out:
conf
```

## Big picture takeaway

There are serious limitations of sentiment analysis using existing lexicons, and you should **think really hard** about your findings and if a lexicon makes sense for your study. Otherwise, word counts and exploration alone can be useful! 

## Your task

Taking this script as a point of departure, apply sentiment analysis on the Game of Thrones. You will find a pdf in the data folder. What are the most common meaningful words and what emotions do you expect will dominate this volume? Are there any terms that are similarly ambiguous to the 'confidence' above? 

### Credits: 
This tutorial is inspired by Allison Horst's Advanced Statistics and Data Analysis.

